# -*- coding: utf-8 -*-
"""Soft Tokens vs Hard Tokens.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19b5l77ktH1IZ1cshRpOPh-UWZY7h6upd
"""

## Why Soft Tokens matter during distillation than hard tokens especially from XBillion parameters to YMillion parameters model##

!pip install transformers accelerate torch --quiet

#load Model
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "tiiuae/falcon-7b-instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model in FP16
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()

# Define prompts
prompt = "What is the relationship between Romeo and Juliet?"
max_length = 50  # max tokens to generate
top_k = 8        # number of soft tokens to store per step
temperature = 1.0

hard_token_ids = []
soft_tokens = []

input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

for step in range(max_length):
    # Forward pass
    with torch.no_grad():
        outputs = model(input_ids)
        logits = outputs.logits  # shape: [batch, seq_len, vocab_size]

    # Take the logits of the last token
    last_logits = logits[0, -1, :]

    # 1️⃣ Hard token (argmax)
    hard_id = torch.argmax(last_logits).item()
    hard_token_ids.append(hard_id)

    # 2️⃣ Soft tokens (top-K)
    probs = torch.softmax(last_logits / temperature, dim=-1)
    topk_probs, topk_ids = torch.topk(probs, top_k)

    # Decode top-K tokens
    topk_tokens = tokenizer.convert_ids_to_tokens(topk_ids.tolist())
    topk_probs = topk_probs.tolist()

    soft_tokens.append({
        "tokens": topk_tokens,
        "probs": topk_probs
    })

    # Append hard token to input_ids for next step
    input_ids = torch.cat([input_ids, torch.tensor([[hard_id]], device=model.device)], dim=1)

    # Stop if EOS token generated
    if hard_id == tokenizer.eos_token_id:
        break

hard_response = tokenizer.decode(hard_token_ids)
print("=== Hard-token response ===")
print(hard_response)

print("\n=== Top-K Soft Tokens per step ===")
for i, step_info in enumerate(soft_tokens):
    tokens = step_info["tokens"]
    probs = step_info["probs"]
    print(f"Step {i+1}:")
    for t, p in zip(tokens, probs):
        print(f"  {t:15} {p:.3f}")
    print("-" * 30)

!pip install matplotlib --quiet

import matplotlib.pyplot as plt

def plot_topk_soft_tokens(soft_tokens, top_k=8):
    """
    soft_tokens: list of dicts
        [{"tokens": [...], "probs": [...]}, ...]
    """
    for step, step_info in enumerate(soft_tokens):
        tokens = step_info["tokens"][:top_k]
        probs = step_info["probs"][:top_k]

        plt.figure(figsize=(6, 2))
        bars = plt.barh(tokens, probs, color='skyblue')
        plt.xlim(0, 1)
        plt.xlabel("Probability")
        plt.title(f"Step {step+1} Top-{top_k} Soft Tokens")

        # Label each bar with its probability
        for bar, prob in zip(bars, probs):
            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                     f"{prob:.3f}", va='center')

        plt.gca().invert_yaxis()  # largest probability on top
        plt.tight_layout()
        plt.show()

plot_topk_soft_tokens(soft_tokens, top_k=8)